{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use cpu........\n",
      "enc_inputs:  torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "sentences = [\n",
    "    # enc_input           dec_input         dec_output\n",
    "    ['ich mochte ein bier P', 'S i want a beer .', 'i want a beer . E'],\n",
    "    ['ich mochte ein cola P', 'S i want a coke .', 'i want a coke . E']\n",
    "]\n",
    "\n",
    "# Padding Should be Zero\n",
    "src_vocab = {'P': 0, 'ich': 1, 'mochte': 2, 'ein': 3, 'bier': 4, 'cola': 5}\n",
    "src_vocab_size = len(src_vocab)\n",
    "\n",
    "tgt_vocab = {'P': 0, 'i': 1, 'want': 2, 'a': 3, 'beer': 4, 'coke': 5, 'S': 6, 'E': 7, '.': 8}\n",
    "idx2word = {i: w for i, w in enumerate(tgt_vocab)}\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "src_len = 5  # enc_input max sequence length\n",
    "tgt_len = 6  # dec_input(=dec_output) max sequence length\n",
    "\n",
    "# Transformer Parameters\n",
    "d_model = 512  # Embedding Size\n",
    "d_ff = 2048  # FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_layers = 6  # number of Encoder of Decoder Layer\n",
    "n_heads = 8  # number of heads in Multi-Head Attention\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"use gpu........\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"use cpu........\")\n",
    "\n",
    "\n",
    "def make_data(sentences):\n",
    "    enc_inputs, dec_inputs, dec_outputs = [], [], []\n",
    "    for i in range(len(sentences)):\n",
    "        enc_input = [[src_vocab[n] for n in sentences[i][0].split()]]  # [[1, 2, 3, 4, 0], [1, 2, 3, 5, 0]]\n",
    "        dec_input = [[tgt_vocab[n] for n in sentences[i][1].split()]]  # [[6, 1, 2, 3, 4, 8], [6, 1, 2, 3, 5, 8]]\n",
    "        dec_output = [[tgt_vocab[n] for n in sentences[i][2].split()]]  # [[1, 2, 3, 4, 8, 7], [1, 2, 3, 5, 8, 7]]\n",
    "\n",
    "        enc_inputs.extend(enc_input)\n",
    "        dec_inputs.extend(dec_input)\n",
    "        dec_outputs.extend(dec_output)\n",
    "\n",
    "    return torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)\n",
    "\n",
    "\n",
    "enc_inputs, dec_inputs, dec_outputs = make_data(sentences)\n",
    "print(\"enc_inputs: \", enc_inputs.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    '''\n",
    "    seq_q: [batch_size, seq_len]\n",
    "    seq_k: [batch_size, seq_len]\n",
    "    seq_len could be src_len or it could be tgt_len\n",
    "    seq_len in seq_q and seq_len in seq_k maybe not equal\n",
    "    '''\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # [batch_size, 1, len_k], False is masked\n",
    "    pad_attn_mask = pad_attn_mask.expand(batch_size, len_q, len_k)\n",
    "    return pad_attn_mask  # [batch_size, len_q, len_k]\n",
    "\n",
    "enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)  # [batch_size, src_len, src_len]\n",
    "self_attn_mask = enc_self_attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "print(self_attn_mask.size())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec_self_attn_mask:\n",
      " tensor([[[False,  True,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True,  True],\n",
      "         [False, False, False,  True,  True,  True],\n",
      "         [False, False, False, False,  True,  True],\n",
      "         [False, False, False, False, False,  True],\n",
      "         [False, False, False, False, False, False]],\n",
      "\n",
      "        [[False,  True,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True,  True],\n",
      "         [False, False, False,  True,  True,  True],\n",
      "         [False, False, False, False,  True,  True],\n",
      "         [False, False, False, False, False,  True],\n",
      "         [False, False, False, False, False, False]]])\n",
      "dec_enc_attn_mask:\n",
      " tensor([[[False, False, False, False,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [False, False, False, False,  True]],\n",
      "\n",
      "        [[False, False, False, False,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [False, False, False, False,  True]]])\n",
      "训练用GT的batch合并: tensor([1, 2, 3, 4, 8, 7, 1, 2, 3, 5, 8, 7])\n",
      "tensor([], size=(1, 0))\n",
      "tensor([[6.]])\n"
     ]
    }
   ],
   "source": [
    "def get_attn_subsequence_mask(seq):\n",
    "    '''\n",
    "    seq: [batch_size, tgt_len]\n",
    "    '''\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    subsequence_mask = np.triu(np.ones(attn_shape), k=1)  # Upper triangular matrix\n",
    "    subsequence_mask = torch.from_numpy(subsequence_mask).byte()\n",
    "    return subsequence_mask  # [batch_size, tgt_len, tgt_len]\n",
    "\n",
    "dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).to(device)\n",
    "dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).to(device)\n",
    "dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequence_mask), 0).to(device)\n",
    "print(\"dec_self_attn_mask:\\n\", dec_self_attn_mask)\n",
    "dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)\n",
    "print(\"dec_enc_attn_mask:\\n\", dec_enc_attn_mask)\n",
    "\n",
    "print(\"训练用GT的batch合并:\", dec_outputs.view(-1))\n",
    "\n",
    "dec_input = torch.zeros(1, 0)\n",
    "print(dec_input)\n",
    "start_symbol=tgt_vocab[\"S\"]\n",
    "dec_input = torch.cat([dec_input.detach(), torch.tensor([[start_symbol]]).to(device)], -1)\n",
    "print(dec_input)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-20580e15",
   "language": "python",
   "display_name": "PyCharm (Workspace_git)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}